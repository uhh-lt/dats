# docker compose -f compose.ray.yml up -d

services:
  ray:
    image: uhhlt/dats_ray:${DATS_RAY_DOCKER_VERSION:-debian_dev_latest}
    ports:
      - ${RAY_API_EXPOSED:-13130}:8000
      - ${RAY_DASHBOARD_EXPOSED:-13131}:8265
    build:
      context: ../backend/src/ray_model_worker
      dockerfile: Dockerfile
    healthcheck:
      test: [ "CMD-SHELL", "python /dats_code_ray/check_ray_health.py" ]
      start_period: 60s
      interval: 60s
      timeout: 60s
      retries: 20
    command: /dats_code_ray/ray_model_worker_entrypoint.sh
    # user: ${UID:-1000}:${GID:-100}
    environment:
      LOG_LEVEL: ${LOG_LEVEL:-info}
      RAY_CONFIG: ${RAY_CONFIG:-config_gpu.yaml}
      SPACY_MODELS_DIR: /ray_cache/spacy_models
      HUGGINGFACE_HUB_CACHE: /ray_cache/hf_hub_cache
      HF_HOME: /ray_cache/hf_cache
      TORCH_HOME: /ray_cache/torch_cache
      NUMBA_CACHE_DIR: /ray_cache/numba_cache
      TORCHINDUCTOR_CACHE_DIR: /ray_cache/torch_inductor_cache
      TRITON_CACHE_DIR: /ray_cache/triton_cache
    volumes:
      # - ../backend/src/ray_model_worker:/dats_code_ray # uncomment for dev
      - "${RAY_CACHE_DIR:-./ray_cache}:/ray_cache"
      #- "${RAY_CONFIG:-../backend/src/ray_model_worker/config_gpu.yaml}:/dats_code_ray/config_gpu.yaml" # uncomment to change config
    shm_size: 12gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "${RAY_DEVICE_IDS:-0}" ]
              capabilities: [ gpu ]
    networks:
      - ray_network

networks:
  ray_network:
    name: ray_network
