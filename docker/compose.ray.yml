# docker compose -f compose.ray.yml up -d

services:
  ray:
    image: uhhlt/dats_ray:${DATS_RAY_DOCKER_VERSION:-debian_dev_latest}
    ports:
      - ${RAY_API_EXPOSED:-13130}:8000
      - ${RAY_DASHBOARD_EXPOSED:-13131}:8265
    build:
      context: ../backend/src/app/preprocessing/ray_model_worker
      dockerfile: Dockerfile
    healthcheck:
      test: [ "CMD-SHELL", "python /dats_code_ray/check_ray_health.py" ]
      start_period: 60s
      interval: 60s
      timeout: 60s
      retries: 20
    command: /dats_code_ray/ray_model_worker_entrypoint.sh
    user: ${UID:-1000}:${GID:-1000}
    environment:
      DATS_BACKEND_CONFIG: /dats_code/src/configs/production.yaml
      LOG_LEVEL: ${LOG_LEVEL:-info}
      RAY_CONFIG: ${RAY_CONFIG:-config_gpu.yaml}
      HUGGINGFACE_HUB_CACHE: /models_cache
      TRANSFORMERS_CACHE: /models_cache
      TORCH_HOME: /models_cache
      NUMBA_CACHE_DIR: /numba_cache
      TORCHINDUCTOR_CACHE_DIR: /torch_inductor_cache
      TRITON_CACHE_DIR: /triton_cache
    volumes:
      # - ../backend/src/app/preprocessing/ray_model_worker:/dats_code_ray  # uncomment for dev
      - "${SPACY_CACHE_DIR:-./spacy_models}:/spacy_models"
      - "${MODELS_CACHE_DIR:-./models_cache}:/models_cache"
      - ./numba_cache:/numba_cache
      - ./torch_inductor_cache:/torch_inductor_cache
      - ./triton_cache:/triton_cache
    shm_size: 12gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "${RAY_DEVICE_IDS:-0}" ]
              capabilities: [ gpu ]
    networks:
      - ray_network

networks:
  ray_network:
    name: ray_network
