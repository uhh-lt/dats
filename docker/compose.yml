services:
  postgres:
    image: "postgres:15-alpine"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-datsuser}" ]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-datsuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dats123}
    volumes:
      - "postgres_data_demo:/var/lib/postgresql/data"
    networks:
      - dats_demo_network

  rabbitmq:
    image: "rabbitmq:3-management-alpine"
    healthcheck:
      test: "rabbitmq-diagnostics check_port_connectivity"
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-datsuser}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-dats123}
    volumes:
      - "rabbitmq_data_demo:/var/lib/rabbitmq/data"
    networks:
      - dats_demo_network

  redis:
    image: "redis:7-alpine"
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli", "ping" ]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dats123}
    volumes:
      - "redis_data_demo:/var/lib/redis/data"
    command: redis-server --requirepass "${REDIS_PASSWORD:-dats123}" --save 60 1
    networks:
      - dats_demo_network

  typesense:
    image: typesense/typesense:0.26.0
    restart: on-failure
    ports:
      - "${TYPESENSE_EXPOSED:-8108}:8108"
    volumes:
      - typesense_data_demo:/data
    environment:
      - TYPESENSE_API_KEY=dats123
      - TYPESENSE_DATA_DIR=/data
    networks:
      - dats_demo_network
    profiles:
      - typesense

  qdrant:
    image: qdrant/qdrant:1.9.2
    ports:
      - "${QDRANT_EXPOSED:-6333}:6333"
      - "${QDRANT_GRPC_EXPOSED:-6334}:6334"
    volumes:
      - qdrant_storage_demo:/qdrant/storage
    networks:
      - dats_demo_network
    profiles:
      - qdrant

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --scheme
      - http
    image: semitechnologies/weaviate:1.21.3
    healthcheck:
      test: [ "CMD-SHELL", "wget -q --spider http://localhost:8080/v1/.well-known/ready || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - "weaviate_data_demo:/var/lib/weaviate"
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: "none"
      CLUSTER_HOSTNAME: "node1"
      LOG_LEVEL: "info"
    networks:
      - dats_demo_network
    profiles:
      - weaviate

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      # see https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html
      # use 127.0.0.1 to access the ES node from OUTSIDE of the docker network (e.g. when ssh tunneling to ltdocker)
      # network.publish_host=127.0.0.1
      # http.publish_port=${ELASTICSEARCH_EXPOSED_PORT:-9200}
      - xpack.security.enabled=false # no auth
      - discovery.type=single-node # single node cluster
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch_data_demo:/usr/share/elasticsearch/data
      - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - dats_demo_network

  ray:
    image: uhhlt/dats_ray:${DATS_RAY_DOCKER_VERSION:-debian_dev_latest}
    build:
      context: ../backend/src/app/preprocessing/ray_model_worker
      dockerfile: Dockerfile
    healthcheck:
      test: [ "CMD-SHELL", "python /dats_code_ray/check_ray_health.py" ]
      start_period: 60s
      interval: 60s
      timeout: 60s
      retries: 20
    command: /dats_code_ray/ray_model_worker_entrypoint.sh
    user: ${UID:-1000}:${GID:-1000}
    environment:
      DATS_BACKEND_CONFIG: /dats_code/src/configs/production.yaml
      LOG_LEVEL: ${LOG_LEVEL:-info}
      RAY_CONFIG: ${RAY_CONFIG:-config_gpu.yaml}
      HUGGINGFACE_HUB_CACHE: /models_cache
      TRANSFORMERS_CACHE: /models_cache
      TORCH_HOME: /models_cache
      NUMBA_CACHE_DIR: /numba_cache
    volumes:
      - ../backend/src/app/preprocessing/ray_model_worker:/dats_code_ray
      - "${SPACY_CACHE_DIR:-./spacy_models}:/spacy_models"
      - ./backend_repo:/tmp/dats
      - "${MODELS_CACHE_DIR:-./models_cache}:/models_cache"
      - ./numba_cache:/numba_cache
    shm_size: 12gb
    networks:
      - dats_demo_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "1" ]
              capabilities: [ gpu ]
    profiles:
      - ray

  celery-background-jobs-worker:
    image: uhhlt/dats_backend:${DATS_BACKEND_DOCKER_VERSION:-debian_dev_latest}
    build:
      context: ../backend
      dockerfile: Dockerfile
    healthcheck:
      test: "/opt/envs/dats/bin/python -m celery -A app.celery.celery_worker inspect ping | grep -q pong || exit 1"
      interval: 30s
      timeout: 10s
      retries: 5
    command: /dats_code/src/celery_background_jobs_worker_entrypoint.sh
    user: ${UID:-1000}:${GID:-1000}
    environment:
      DATS_BACKEND_CONFIG: /dats_code/src/configs/production.yaml
      LOG_LEVEL: ${CELERY_LOG_LEVEL:-info}
      CELERY_BACKGROUND_JOBS_WORKER_CONCURRENCY: ${CELERY_BACKGROUND_JOBS_WORKER_CONCURRENCY:-1}
      POSTGRES_DB: ${POSTGRES_DB:-dats}
      POSTGRES_USER: ${POSTGRES_USER:-datsuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dats123}
      RABBITMQ_USER: ${RABBITMQ_USER:-datsuser}
      RABBITMQ_PASSWORD: ${RABBITMQ_PASSWORD:-dats123}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dats123}
      RAY_HOST: ${RAY_HOST:-ray}
      RAY_PORT: ${RAY_PORT:-8000}
      RAY_PROTOCOL: ${RAY_PROTOCOL:-http}
      OLLAMA_HOST: ${OLLAMA_HOST:-ollama}
      OLLAMA_PORT: ${OLLAMA_PORT:-11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gemma2:9b-instruct-fp16}
      HUGGINGFACE_HUB_CACHE: /models_cache
      TRANSFORMERS_CACHE: /models_cache
      TORCH_HOME: /models_cache
      NUMBA_CACHE_DIR: /numba_cache
    volumes:
      - ../backend/src:/dats_code/src
      - ./backend_repo:/tmp/dats
      - "${MODELS_CACHE_DIR:-./models_cache}:/models_cache"
      - ./numba_cache:/numba_cache
    depends_on:
      postgres:
        condition: service_healthy
        restart: true
      rabbitmq:
        condition: service_healthy
        restart: true
      redis:
        condition: service_healthy
        restart: true
      weaviate:
        condition: service_healthy
        restart: true
      elasticsearch:
        condition: service_healthy
        restart: true
      ray:
        condition: service_healthy
        restart: true
    networks:
      - dats_demo_network
      - ollama_network
    profiles:
      - background

  dats-backend-api:
    image: uhhlt/dats_backend:${DATS_BACKEND_DOCKER_VERSION:-debian_dev_latest}
    build:
      context: ../backend
      dockerfile: Dockerfile
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:5500 || exit 1" ]
      start_period: 60s
      interval: 60s
      timeout: 10s
      retries: 5
    command: /dats_code/src/backend_api_entrypoint.sh
    user: ${UID:-1000}:${GID:-1000}
    environment:
      DATS_BACKEND_CONFIG: /dats_code/src/configs/production.yaml
      LOG_LEVEL: ${API_LOG_LEVEL:-info}
      API_WORKERS: ${API_WORKERS:-10}
      JWT_SECRET: ${JWT_SECRET:-""}
      SYSTEM_USER_EMAIL: ${SYSTEM_USER_EMAIL}
      SYSTEM_USER_PASSWORD: ${SYSTEM_USER_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-dats}
      POSTGRES_USER: ${POSTGRES_USER:-datsuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dats123}
      RABBITMQ_USER: ${RABBITMQ_USER:-datsuser}
      RABBITMQ_PASSWORD: ${RABBITMQ_PASSWORD:-dats123}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dats123}
      RAY_HOST: ${RAY_HOST:-ray}
      RAY_PORT: ${RAY_PORT:-8000}
      RAY_PROTOCOL: ${RAY_PROTOCOL:-http}
      OLLAMA_HOST: ${OLLAMA_HOST:-ollama}
      OLLAMA_PORT: ${OLLAMA_PORT:-11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gemma2:9b-instruct-fp16}
    volumes:
      - ../backend/src:/dats_code/src
      - ./backend_repo:/tmp/dats
    depends_on:
      postgres:
        condition: service_healthy
        restart: true
      rabbitmq:
        condition: service_healthy
        restart: true
      redis:
        condition: service_healthy
        restart: true
      weaviate:
        condition: service_healthy
        restart: true
      elasticsearch:
        condition: service_healthy
        restart: true
      ray:
        condition: service_healthy
        restart: true
      celery-background-jobs-worker:
        condition: service_healthy
        restart: true
    ports:
      - "${API_EXPOSED:-13120}:5500"
    networks:
      - dats_demo_network
      - ollama_network
    profiles:
      - backend

  dats-frontend:
    image: uhhlt/dats_frontend:${DATS_FRONTEND_DOCKER_VERSION:-latest}
    build:
      context: ../frontend
      dockerfile: Dockerfile
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:3000 || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./backend_repo:/usr/share/nginx/content:ro
    depends_on:
      dats-backend-api:
        condition: service_healthy
        restart: true
    ports:
      - "${FRONTEND_EXPOSED:-13100}:3000"
    networks:
      - dats_demo_network
    profiles:
      - frontend

volumes:
  rabbitmq_data_demo:
    driver: local
  redis_data_demo:
    driver: local
  postgres_data_demo:
    driver: local
  elasticsearch_data_demo:
    driver: local
  weaviate_data_demo:
    driver: local
  typesense_data_demo:
    driver: local

networks:
  dats_demo_network:
  ollama_network:
    name: ollama_network
    external: true
