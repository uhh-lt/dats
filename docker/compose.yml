x-airflow-common: &airflow-common
  image: uhhlt/dats_airflow:${DATS_AIRFLOW_DOCKER_VERSION:?dats-airflow-docker-version-required!}
  build:
    context: ../
    dockerfile: airflow/Dockerfile
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:?postgres-user-required!}:${POSTGRES_PASSWORD:?postgres-password-required!}@postgres/${AIRFLOW_DB:?airflow-db-required!}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:?postgres-user-required!}:${POSTGRES_PASSWORD:?postgres-password-required!}@postgres/${AIRFLOW_DB:?airflow-db-required!}
    AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD:?redis-password-required!}@redis:6379/${AIRFLOW_REDIS_DB:?airflow-redis-db-required!}
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow-apiserver:8080/execution/"
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
    AIRFLOW_CONFIG: "/opt/airflow/config/airflow.cfg"
  volumes:
    - ./airflow_cache:/opt/airflow/logs
    # - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    # - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  user: "${DOCKER_UID:?docker-uid-required}:0"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy
  networks:
    - dats_network
  profiles:
    - airflow

x-dats-common: &dats-common
  image: uhhlt/dats_backend:${DATS_BACKEND_DOCKER_VERSION:?dats-backend-docker-version-required!}
  build:
    context: ../backend
    dockerfile: Dockerfile
  user: ${DOCKER_UID:-1000}:${DOCKER_GID:-1000}
  environment: &dats-common-env
    DATS_BACKEND_MODE: production
    JWT_SECRET: ${JWT_SECRET:?jwt secret required}
    SESSION_SECRET: ${SESSION_SECRET:?session secret required}
    UUID_NAMESPACE: ${UUID_NAMESPACE:?uuid namespace required}
    SYSTEM_USER_EMAIL: ${SYSTEM_USER_EMAIL}
    SYSTEM_USER_PASSWORD: ${SYSTEM_USER_PASSWORD}
    POSTGRES_DB: ${DATS_DB:-dats}
    POSTGRES_USER: ${POSTGRES_USER:-datsuser}
    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dats123}
    RABBITMQ_USER: ${RABBITMQ_USER:-datsuser}
    RABBITMQ_PASSWORD: ${RABBITMQ_PASSWORD:-dats123}
    REDIS_PASSWORD: ${REDIS_PASSWORD:-dats123}
    OLLAMA_HOST: ${OLLAMA_HOST:-ollama}
    OLLAMA_PORT: ${OLLAMA_PORT:-11434}
    OLLAMA_LLM_MODEL: ${OLLAMA_LLM_MODEL:-gemma3:27b}
    OLLAMA_VLM_MODEL: ${OLLAMA_VLM_MODEL:-gemma3:27b}
    OLLAMA_EMB_MODEL: ${OLLAMA_EMB_MODEL:-snowflake-arctic-embed2:568m}
    RAY_HOST: ${RAY_HOST:-ray}
    RAY_PORT: ${RAY_PORT:-8000}
    OIDC_ENABLED: ${OIDC_ENABLED:-false}
    OIDC_PROVIDER_NAME: ${OIDC_PROVIDER_NAME:-Authentik}
    OIDC_CLIENT_ID: ${OIDC_CLIENT_ID}
    OIDC_CLIENT_SECRET: ${OIDC_CLIENT_SECRET}
    OIDC_SERVER_METADATA_URL: ${OIDC_SERVER_METADATA_URL}
    IS_STABLE: ${IS_STABLE:-false}
  depends_on: &dats-common-depends-on
    postgres:
      condition: service_healthy
      restart: true
    rabbitmq:
      condition: service_healthy
      restart: true
    redis:
      condition: service_healthy
      restart: true
    weaviate:
      condition: service_healthy
      restart: true
    elasticsearch:
      condition: service_healthy
      restart: true
  networks:
    - dats_network
    - ollama_network
    - ray_network

services:
  postgres:
    image: postgres:15-alpine
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER:?postgres-user-required!}"]
      interval: 10s
      retries: 5
      start_period: 5s
    environment:
      POSTGRES_USER: ${POSTGRES_USER:?postgres-user-required!}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?postgres-password-required!}
      DATS_DB: ${DATS_DB:?dats-db-required!}
      AIRFLOW_DB: ${AIRFLOW_DB:?airflow-db-required!}
    volumes:
      - ./configs/postgres/init-dbs.sh:/docker-entrypoint-initdb.d/init-dbs.sh
    restart: always
    networks:
      - dats_network

  rabbitmq:
    image: "rabbitmq:3-management-alpine"
    healthcheck:
      test: "rabbitmq-diagnostics check_port_connectivity"
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-datsuser}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-dats123}
    networks:
      - dats_network

  redis:
    image: redis:7.2-bookworm
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:?redis-password-required!}
    command: redis-server --requirepass "${REDIS_PASSWORD:?redis-password-required!}" --save 60 1
    restart: always
    networks:
      - dats_network

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --scheme
      - http
    image: semitechnologies/weaviate:1.30.4
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/v1/.well-known/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - "../backups/weaviate:/mount/backups"
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: "none"
      CLUSTER_HOSTNAME: "node1"
      LOG_LEVEL: "info"
      ENABLE_MODULES: "backup-filesystem"
      BACKUP_FILESYSTEM_PATH: "/mount/backups"
    networks:
      - dats_network

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    user: ${DOCKER_UID:-1000}:${DOCKER_GID:-1000}
    group_add:
      - "0"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      # see https://www.elastic.co/gDOCKER_uide/en/elasticsearch/reference/current/modules-network.html
      # use 127.0.0.1 to access the ES node from OUTSIDE of the docker network (e.g. when ssh tunneling to ltdocker)
      # network.publish_host=127.0.0.1
      # http.publish_port=${ELASTICSEARCH_EXPOSED_PORT:-9200}
      - xpack.security.enabled=false # no auth
      - discovery.type=single-node # single node cluster
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./configs/es/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - ../backups/elasticsearch:/mount/backups
    networks:
      - dats_network

  rq-worker:
    <<: *dats-common
    healthcheck:
      test: "/dats_code/.venv/bin/python /dats_code/src/worker.py healthcheck"
      interval: 30s
      timeout: 10s
      retries: 5
    command: /dats_code/src/rq_worker_entrypoint.sh
    environment:
      <<: *dats-common-env
      NUM_RQ_WORKER_CPU: ${RQ_NUM_RQ_WORKER_CPU:-8}
      NUM_RQ_WORKER_GPU: ${RQ_NUM_RQ_WORKER_GPU:-16}
      LOG_LEVEL: ${RQ_LOG_LEVEL:-info}
      NUMBA_CACHE_DIR: /rq_cache/numba_cache
      MPLCONFIGDIR: /rq_cache/matplotlib
    volumes:
      - ./backend_repo:/tmp/dats
      - ./rq_cache:/rq_cache
    profiles:
      - rq

  rq-dashboard:
    image: cjlapao/rq-dashboard:0.8.4
    environment:
      RQ_DASHBOARD_REDIS_URL: redis://:${REDIS_PASSWORD:-dats123}@redis:6379/10
      RQ_DASHBOARD_USERNAME: ${RQ_DASHBOARD_USERNAME:-datsuser}
      RQ_DASHBOARD_PASSWORD: ${RQ_DASHBOARD_PASSWORD:-dats123}
    depends_on:
      rq-worker:
        condition: service_healthy
        restart: true
    ports:
      - "${RQ_DASHBOARD_EXPOSED:-13136}:9181"
    networks:
      - dats_network
    profiles:
      - rq

  airflow-apiserver:
    <<: *airflow-common
    command: api-server
    ports:
      - "${AIRFLOW_EXPOSED:?airflow-exposed-required!}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: [*airflow-common-env, *dats-common-env]
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
      # DATS Environment Variables:
      NUMBA_CACHE_DIR: /airflow_cache/numba_cache
      MPLCONFIGDIR: /airflow_cache/matplotlib
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-apiserver:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command: ["/opt/airflow-init.sh"]
    volumes:
      - ./airflow_cache:/opt/airflow/logs
      - ./configs/airflow/airflow-init.sh:/opt/airflow-init.sh:ro
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    user: "0:0"

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow
    depends_on:
      <<: *airflow-common-depends-on

  dats-backend-api:
    <<: *dats-common
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5500 || exit 1"]
      start_period: 60s
      interval: 60s
      timeout: 10s
      retries: 5
    command: /dats_code/src/backend_api_entrypoint.sh
    environment:
      <<: *dats-common-env
      LOG_LEVEL: ${API_LOG_LEVEL:-info}
      API_WORKERS: ${API_WORKERS:-10}
      NUMBA_CACHE_DIR: /api_cache/numba_cache
      MPLCONFIGDIR: /api_cache/matplotlib
    volumes:
      - ./backend_repo:/tmp/dats
      - ./api_cache:/api_cache
    depends_on:
      <<: *dats-common-depends-on
      celery-background-jobs-worker:
        condition: service_healthy
        restart: true
      rq-worker:
        condition: service_healthy
        restart: true
    ports:
      - "${API_EXPOSED:-13120}:5500"
    profiles:
      - backend

  dats-frontend:
    image: uhhlt/dats_frontend:${DATS_FRONTEND_DOCKER_VERSION:-latest}
    build:
      context: ../frontend
      dockerfile: Dockerfile
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - ./configs/frontend/logo.png:/usr/share/nginx/html/logo2.png
      - ./configs/frontend/nginx.conf:/etc/nginx/nginx.conf
      - ./backend_repo:/usr/share/nginx/content:ro
    depends_on:
      dats-backend-api:
        condition: service_healthy
        restart: true
    ports:
      - "${FRONTEND_EXPOSED:-13100}:3000"
    networks:
      - dats_network
    profiles:
      - frontend

networks:
  dats_network:
  ollama_network:
    name: ollama_network
    external: true
  ray_network:
    name: ray_network
    external: true
