services:
  postgres:
    image: "postgres:15-alpine"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-datsuser}" ]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-datsuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dats123}
    networks:
      - dats_network

  rabbitmq:
    image: "rabbitmq:3-management-alpine"
    healthcheck:
      test: "rabbitmq-diagnostics check_port_connectivity"
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-datsuser}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-dats123}
    networks:
      - dats_network

  redis:
    image: "redis:7-alpine"
    healthcheck:
      test: [ "CMD-SHELL", "redis-cli", "ping" ]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dats123}
    command: redis-server --requirepass "${REDIS_PASSWORD:-dats123}" --save 60 1
    networks:
      - dats_network

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - "8080"
      - --scheme
      - http
    image: semitechnologies/weaviate:1.30.4
    healthcheck:
      test: [ "CMD-SHELL", "wget -q --spider http://localhost:8080/v1/.well-known/ready || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - "../backups/weaviate:/mount/backups"
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: "none"
      CLUSTER_HOSTNAME: "node1"
      LOG_LEVEL: "info"
      ENABLE_MODULES: "backup-filesystem"
      BACKUP_FILESYSTEM_PATH: "/mount/backups"
    networks:
      - dats_network
    profiles:
      - weaviate

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    user: ${DOCKER_UID:-1000}:${DOCKER_GID:-1000}
    group_add:
      - "0"
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      # see https://www.elastic.co/gDOCKER_uide/en/elasticsearch/reference/current/modules-network.html
      # use 127.0.0.1 to access the ES node from OUTSIDE of the docker network (e.g. when ssh tunneling to ltdocker)
      # network.publish_host=127.0.0.1
      # http.publish_port=${ELASTICSEARCH_EXPOSED_PORT:-9200}
      - xpack.security.enabled=false # no auth
      - discovery.type=single-node # single node cluster
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - ../backups/elasticsearch:/mount/backups
    networks:
      - dats_network

  celery-background-jobs-worker:
    image: uhhlt/dats_backend:${DATS_BACKEND_DOCKER_VERSION:-debian_dev_latest}
    build:
      context: ../backend
      dockerfile: Dockerfile
    healthcheck:
      test: "/dats_code/.venv/bin/python -m celery -A app.celery.celery_worker inspect ping | grep -q pong || exit 1"
      interval: 30s
      timeout: 10s
      retries: 5
    command: /dats_code/src/celery_background_jobs_worker_entrypoint.sh
    user: ${DOCKER_UID:-1000}:${DOCKER_GID:-1000}
    environment:
      DATS_BACKEND_CONFIG: /dats_code/src/configs/production.yaml
      LOG_LEVEL: ${CELERY_LOG_LEVEL:-info}
      CELERY_BACKGROUND_JOBS_WORKER_CONCURRENCY: ${CELERY_BACKGROUND_JOBS_WORKER_CONCURRENCY:-1}
      UUID_NAMESPACE: ${UUID_NAMESPACE:?uuid namespace required}
      POSTGRES_DB: ${POSTGRES_DB:-dats}
      POSTGRES_USER: ${POSTGRES_USER:-datsuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dats123}
      RABBITMQ_USER: ${RABBITMQ_USER:-datsuser}
      RABBITMQ_PASSWORD: ${RABBITMQ_PASSWORD:-dats123}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dats123}
      OLLAMA_HOST: ${OLLAMA_HOST:-ollama}
      OLLAMA_PORT: ${OLLAMA_PORT:-11434}
      OLLAMA_LLM_MODEL: ${OLLAMA_LLM_MODEL:-gemma3:27b}
      OLLAMA_VLM_MODEL: ${OLLAMA_VLM_MODEL:-gemma3:27b}
      OLLAMA_EMB_MODEL: ${OLLAMA_EMB_MODEL:-snowflake-arctic-embed2:568m}
      RAY_HOST: ${RAY_HOST:-ray}
      RAY_PORT: ${RAY_PORT:-8000}
      NUMBA_CACHE_DIR: /celery_cache/numba_cache
    volumes:
      - ./backend_repo:/tmp/dats
      - ./celery_cache:/celery_cache
    depends_on:
      postgres:
        condition: service_healthy
        restart: true
      rabbitmq:
        condition: service_healthy
        restart: true
      redis:
        condition: service_healthy
        restart: true
      weaviate:
        condition: service_healthy
        restart: true
      elasticsearch:
        condition: service_healthy
        restart: true
    networks:
      - dats_network
      - ollama_network
      - ray_network
    profiles:
      - background

  dats-backend-api:
    image: uhhlt/dats_backend:${DATS_BACKEND_DOCKER_VERSION:-debian_dev_latest}
    build:
      context: ../backend
      dockerfile: Dockerfile
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:5500 || exit 1" ]
      start_period: 60s
      interval: 60s
      timeout: 10s
      retries: 5
    command: /dats_code/src/backend_api_entrypoint.sh
    user: ${DOCKER_UID:-1000}:${DOCKER_GID:-1000}
    environment:
      DATS_BACKEND_CONFIG: /dats_code/src/configs/production.yaml
      LOG_LEVEL: ${API_LOG_LEVEL:-info}
      API_WORKERS: ${API_WORKERS:-10}
      JWT_SECRET: ${JWT_SECRET:?jwt secret required}
      SESSION_SECRET: ${SESSION_SECRET:?session secret required}
      UUID_NAMESPACE: ${UUID_NAMESPACE:?uuid namespace required}
      SYSTEM_USER_EMAIL: ${SYSTEM_USER_EMAIL}
      SYSTEM_USER_PASSWORD: ${SYSTEM_USER_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-dats}
      POSTGRES_USER: ${POSTGRES_USER:-datsuser}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dats123}
      RABBITMQ_USER: ${RABBITMQ_USER:-datsuser}
      RABBITMQ_PASSWORD: ${RABBITMQ_PASSWORD:-dats123}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-dats123}
      OLLAMA_HOST: ${OLLAMA_HOST:-ollama}
      OLLAMA_PORT: ${OLLAMA_PORT:-11434}
      OLLAMA_LLM_MODEL: ${OLLAMA_LLM_MODEL:-gemma3:27b}
      OLLAMA_VLM_MODEL: ${OLLAMA_VLM_MODEL:-gemma3:27b}
      OLLAMA_EMB_MODEL: ${OLLAMA_EMB_MODEL:-snowflake-arctic-embed2:568m}
      RAY_HOST: ${RAY_HOST:-ray}
      RAY_PORT: ${RAY_PORT:-8000}
      OIDC_ENABLED: ${OIDC_ENABLED:-false}
      OIDC_PROVIDER_NAME: ${OIDC_PROVIDER_NAME:-Authentik}
      OIDC_CLIENT_ID: ${OIDC_CLIENT_ID}
      OIDC_CLIENT_SECRET: ${OIDC_CLIENT_SECRET}
      OIDC_SERVER_METADATA_URL: ${OIDC_SERVER_METADATA_URL}
      IS_STABLE: ${IS_STABLE:-false}
    volumes:
      - ./backend_repo:/tmp/dats
    depends_on:
      postgres:
        condition: service_healthy
        restart: true
      rabbitmq:
        condition: service_healthy
        restart: true
      redis:
        condition: service_healthy
        restart: true
      weaviate:
        condition: service_healthy
        restart: true
      elasticsearch:
        condition: service_healthy
        restart: true
      celery-background-jobs-worker:
        condition: service_healthy
        restart: true
    ports:
      - "${API_EXPOSED:-13120}:5500"
    networks:
      - dats_network
      - ollama_network
      - ray_network
    profiles:
      - backend

  dats-frontend:
    image: uhhlt/dats_frontend:${DATS_FRONTEND_DOCKER_VERSION:-latest}
    build:
      context: ../frontend
      dockerfile: Dockerfile
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:3000 || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - ./logo.png:/usr/share/nginx/html/logo2.png
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./backend_repo:/usr/share/nginx/content:ro
    depends_on:
      dats-backend-api:
        condition: service_healthy
        restart: true
    ports:
      - "${FRONTEND_EXPOSED:-13100}:3000"
    networks:
      - dats_network
    profiles:
      - frontend

networks:
  dats_network:
  ollama_network:
    name: ollama_network
    external: true
  ray_network:
    name: ray_network_tim
    external: true
